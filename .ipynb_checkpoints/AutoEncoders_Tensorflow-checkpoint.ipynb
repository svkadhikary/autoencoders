{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37dbdc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee195df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets import the dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd5fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the data\n",
    "x_train = x_train.astype('float32')/255.\n",
    "x_test = x_test.astype('float32')/255.\n",
    "# reshaping the data\n",
    "x_image_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_image_train = tf.cast(x_image_train, 'float32')\n",
    "x_image_test = x_test.reshape(-1, 28, 28, 1)\n",
    "x_image_test = tf.cast(x_image_test, 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f88fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(tf.keras.Model):\n",
    "    def __init__(self, n_hidden_1, n_hidden_2, n_encoding, n_input):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # initialize layers\n",
    "#         self.flatten = tf.keras.layers.Flatten()\n",
    "        self.encoding_1 = tf.keras.layers.Dense(n_hidden_1, activation=tf.nn.sigmoid)\n",
    "        self.encoding_2 = tf.keras.layers.Dense(n_hidden_2, activation=tf.nn.sigmoid)\n",
    "        self.encoding_final = tf.keras.layers.Dense(n_encoding, activation=tf.nn.relu)\n",
    "        self.decoding_1 = tf.keras.layers.Dense(n_hidden_2, activation=tf.nn.sigmoid)\n",
    "        self.decoding_2 = tf.keras.layers.Dense(n_hidden_1, activation=tf.nn.sigmoid)\n",
    "        self.decoding_final = tf.keras.layers.Dense(n_input)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        # encoding layer\n",
    "#         x = self.flatten(x)\n",
    "        x = self.encoding_1(x)\n",
    "        x = self.encoding_2(x)\n",
    "        x = self.encoding_final(x)\n",
    "        return x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        # decoding layer\n",
    "        x = self.decoding_1(x)\n",
    "        x = self.decoding_2(x)\n",
    "        x = self.decoding_final(x)\n",
    "        return x\n",
    "    \n",
    "    def call(self, x):\n",
    "        # calling encoding and decoding layer\n",
    "        encoder_op = self.encoder(x)\n",
    "        # Reconstructed image\n",
    "        y_pred = self.decoder(encoder_op)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "219b6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y_pred, y_true):\n",
    "    # cost function\n",
    "    loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52e065a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model, inputs, targets):\n",
    "    # gradient calc function\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstruction = model(inputs)\n",
    "        loss_val = cost(reconstruction, targets)\n",
    "        grads = tape.gradient(loss_val, model.trainable_variables)\n",
    "    return loss_val, grads, reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78b7a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(lr):\n",
    "    return tf.keras.optimizers.RMSprop(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "485bfc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize input shapes for data, hidden layers, batch size, epochs and learning rate\n",
    "n_input = 28*28\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 128\n",
    "n_encoding = 32\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "total_batch = int(len(x_image_train)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9dfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten layer\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "x_train = flatten(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8a0424d",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__SquaredDifference_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:SquaredDifference]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_batch):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# divide training data according to batch size\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     x_input \u001b[38;5;241m=\u001b[39m x_train[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m---> 10\u001b[0m     loss, grads, reconstruction \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     opt\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(model, inputs, targets)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m      4\u001b[0m     reconstruction \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m----> 5\u001b[0m     loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mcost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss_val, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_val, grads, reconstruction\n",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m, in \u001b[0;36mcost\u001b[1;34m(y_pred, y_true)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost\u001b[39m(y_pred, y_true):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# cost function\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     cost \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(loss)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cost\n",
      "File \u001b[1;32mF:\\Users\\Svk\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mF:\\Users\\Svk\\miniconda3\\envs\\tf2\\lib\\site-packages\\keras\\losses.py:1486\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m   1484\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_pred)\n\u001b[0;32m   1485\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(y_true, y_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 1486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mmean(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquared_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__SquaredDifference_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:SquaredDifference]"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model = AutoEncoder(n_hidden_1, n_hidden_2, n_encoding, n_input)\n",
    "# get optimizer\n",
    "opt = optimizer(lr)\n",
    "# run training\n",
    "for epoch in range(epochs):\n",
    "    for i in range(total_batch):\n",
    "        # divide training data according to batch size\n",
    "        x_input = x_train[i:i+batch_size]\n",
    "        loss, grads, reconstruction = grad(model, x_input, x_input)\n",
    "        opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch: {epoch} --- Cost: {loss}\")\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14ec99ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60000, 784])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb45c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
